{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Spacy is one of the leading libraries for Natural Language Processing in Python, It comes with different models\n",
    "###### Like, in the code below we are loading the model spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Processing the below text after loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Oh, ma boy ain't got time to deal with you.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### With this Doc object you can do multiple things with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### It is the process of returning all specific units of all the individual text that has be provided\n",
    "###### It actually returns document object that contains tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterating the tokens through the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh\n",
      ",\n",
      "ma\n",
      "boy\n",
      "ai\n",
      "n't\n",
      "got\n",
      "time\n",
      "to\n",
      "deal\n",
      "with\n",
      "you\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can see that it is also seperating ain't to (ai) and (n't) into different tokens and puntuations as well.\n",
    "Each token comes with additional information of itself such is token.lemma , token.is_stop etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token.is_stop returns either True or False, that whether that token is a stop word or not.\n",
    "Stop Words are actually frequently occuring words in a sentence or a paragraph or any text bigger then a paragraph \n",
    "stop words are ==> is, the, but, not, how, or what. Stop Words usually doesn't contains much information.\n",
    "Removing Stop Words is considered as a best practice with Text Processing or sometimes it is considered as the process of hyperparameter optimization process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization / Lemmatizing / Lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of lemmatization is actually a process of converting a text into it's base form. Such as the Lemma of 'walking' is ==> 'walk', lemma 'eating' is ==> 'eat' etc. \n",
    "Lemmatizating of words is also considered as the common practice with prcocessing of a text or sometimes it is considered as the process of hyperparameter optimization process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The job of feets is walking, but their hobby is dancing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\t\t\t\tLemma\t\t\t\tStopWords\n",
      "____________________________________________________________________________________________________\n",
      "The\t\t\t\tthe\t\t\t\tTrue \n",
      "job\t\t\t\tjob\t\t\t\tFalse \n",
      "of\t\t\t\tof\t\t\t\tTrue \n",
      "feets\t\t\t\tfeet\t\t\t\tFalse \n",
      "is\t\t\t\tbe\t\t\t\tTrue \n",
      "walking\t\t\t\twalk\t\t\t\tFalse \n",
      ",\t\t\t\t,\t\t\t\tFalse \n",
      "but\t\t\t\tbut\t\t\t\tTrue \n",
      "their\t\t\t\t-PRON-\t\t\t\tTrue \n",
      "hobby\t\t\t\thobby\t\t\t\tFalse \n",
      "is\t\t\t\tbe\t\t\t\tTrue \n",
      "dancing\t\t\t\tdance\t\t\t\tFalse \n",
      ".\t\t\t\t.\t\t\t\tFalse \n"
     ]
    }
   ],
   "source": [
    "print('Token\\t\\t\\t\\tLemma\\t\\t\\t\\tStopWords')\n",
    "print('_'*100)\n",
    "for token in doc:\n",
    "    print(\"{t}\\t\\t\\t\\t{l}\\t\\t\\t\\t{s} \".format(t=token, l=token.lemma_, s=token.is_stop)) \n",
    "    # \\t ==> adding 4 spaces are the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pattern matching is also one of the most common process to be done in NLP (Natural Language processing). If you want to create pattern matching of tokens then you create a matcher. If you want to match list of terms, so it is easier and more efficient to use PhraseMatcher.\n",
    "Exp, if you want to find that where these smartphones appears in some text then you create a patterns for the model names of your interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab ,attr='LOWER' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the matcher has been created using the vocabulary of your model that was loaded earlier. and attr = 'lower' has been provided to avoid case sensitive errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now you create a list of patterns(mobile names) that you want to find and I am putting this in the terms variables. Then you have to convert them into document objects which is required by the Phrase matcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\n",
    "patterns = [nlp(text) for text in terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Galaxy Note, iPhone 11, iPhone XS, Google Pixel]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"UsmanTerminologyList\",None,*patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you create a document from the text to search and uses the PhraseMatcher to search that from the the terms occur in the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n",
    "               \"photography tests pitting the iPhone 11 Pro against the \"\n",
    "               \"Galaxy Note 10 Plus and last yearâ€™s iPhone XS and Google Pixel 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(text_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matches here are a tuple of the match id and the positions of the start and end of the phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8556725160488194001, 17, 19),\n",
       " (8556725160488194001, 22, 24),\n",
       " (8556725160488194001, 30, 32),\n",
       " (8556725160488194001, 33, 35)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_id,start,end = matches[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UsmanTerminologyList Galaxy Note\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab.strings[match_id],text_doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit0fa2055d49784cf6a0068ccd845e7b34"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
